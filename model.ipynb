{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Input,merge\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Deconvolution2D, Convolution2D,UpSampling2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.engine import InputSpec\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self,\n",
    "                 padding=(1, 1),\n",
    "                 dim_ordering='default',\n",
    "                 **kwargs):\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "\n",
    "        self.padding = padding\n",
    "        if isinstance(padding, dict):\n",
    "            if set(padding.keys()) <= {'top_pad', 'bottom_pad', 'left_pad', 'right_pad'}:\n",
    "                self.top_pad = padding.get('top_pad', 0)\n",
    "                self.bottom_pad = padding.get('bottom_pad', 0)\n",
    "                self.left_pad = padding.get('left_pad', 0)\n",
    "                self.right_pad = padding.get('right_pad', 0)\n",
    "            else:\n",
    "                raise ValueError('Unexpected key found in `padding` dictionary. '\n",
    "                                 'Keys have to be in {\"top_pad\", \"bottom_pad\", '\n",
    "                                 '\"left_pad\", \"right_pad\"}.'\n",
    "                                 'Found: ' + str(padding.keys()))\n",
    "        else:\n",
    "            padding = tuple(padding)\n",
    "            if len(padding) == 2:\n",
    "                self.top_pad = padding[0]\n",
    "                self.bottom_pad = padding[0]\n",
    "                self.left_pad = padding[1]\n",
    "                self.right_pad = padding[1]\n",
    "            elif len(padding) == 4:\n",
    "                self.top_pad = padding[0]\n",
    "                self.bottom_pad = padding[1]\n",
    "                self.left_pad = padding[2]\n",
    "                self.right_pad = padding[3]\n",
    "            else:\n",
    "                raise TypeError('`padding` should be tuple of int '\n",
    "                                'of length 2 or 4, or dict. '\n",
    "                                'Found: ' + str(padding))\n",
    "\n",
    "        if dim_ordering not in {'tf'}:\n",
    "            raise ValueError('dim_ordering must be in {tf}.')\n",
    "        self.dim_ordering = dim_ordering\n",
    "        self.input_spec = [InputSpec(ndim=4)] \n",
    "\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        top_pad=self.top_pad\n",
    "        bottom_pad=self.bottom_pad\n",
    "        left_pad=self.left_pad\n",
    "        right_pad=self.right_pad        \n",
    "        \n",
    "        paddings = [[0,0],[left_pad,right_pad],[top_pad,bottom_pad],[0,0]]\n",
    "\n",
    "        \n",
    "        return tf.pad(x,paddings, mode='REFLECT', name=None)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if self.dim_ordering == 'tf':\n",
    "            rows = input_shape[1] + self.top_pad + self.bottom_pad if input_shape[1] is not None else None\n",
    "            cols = input_shape[2] + self.left_pad + self.right_pad if input_shape[2] is not None else None\n",
    "\n",
    "            return (input_shape[0],\n",
    "                    rows,\n",
    "                    cols,\n",
    "                    input_shape[3])\n",
    "        else:\n",
    "            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n",
    "            \n",
    "    def get_config(self):\n",
    "        config = {'padding': self.padding}\n",
    "        base_config = super(ReflectionPadding2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_bn_relu(nb_filter, nb_row, nb_col,stride):   \n",
    "    def conv_func(x):\n",
    "        x = Convolution2D(nb_filter, nb_row, nb_col, subsample=stride,border_mode='same')(x)\n",
    "        x = BatchNormalization(mode=1)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "    return conv_func    \n",
    "\n",
    "\n",
    "\n",
    "#https://keunwoochoi.wordpress.com/2016/03/09/residual-networks-implementation-on-keras/\n",
    "def res(nb_filter, nb_row, nb_col,stride=(1,1)):   \n",
    "    def _shortcut(input):\n",
    "        shortcut = Convolution2D(nb_filter=128, nb_row=3, nb_col=3, subsample=(1, 1), border_mode='valid')(input)\n",
    "        shortcut = Convolution2D(nb_filter=128, nb_row=3, nb_col=3, subsample=(1, 1), border_mode='valid')(shortcut)\n",
    "        return shortcut   \n",
    "    \n",
    "    def _res_func(x):\n",
    "        a = Convolution2D(nb_filter, nb_row, nb_col, subsample=stride, border_mode='valid')(x)\n",
    "        a = BatchNormalization(mode=1)(a)\n",
    "        a = Activation('relu')(a)\n",
    "        a = Convolution2D(nb_filter, nb_row, nb_col, subsample=stride, border_mode='valid')(a)\n",
    "        y = BatchNormalization(mode=1)(a)\n",
    "        \n",
    "        shortcut = _shortcut(x)\n",
    "        \n",
    "        return merge([shortcut, y], mode='sum')\n",
    "    \n",
    "    return _res_func    \n",
    "\n",
    "\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "\n",
    "    \n",
    "def dconv_bn_relu(nb_filter, nb_row, nb_col,stride=(2,2)):   \n",
    "    def dconv_bn_relu(x):\n",
    "        x = UpSampling2D(size=stride)(x)\n",
    "        x = Convolution2D(nb_filter,nb_row, nb_col, border_mode='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "    return dconv_bn_relu        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def image_transform_netl():\n",
    "    x = Input(shape=(256,256,3))\n",
    "    a = ReflectionPadding2D(padding=(40,40),input_shape=(256,256,3))(x)\n",
    "    a = conv_bn_relu(32, 9, 9, stride=(1,1))(a)\n",
    "    a = conv_bn_relu(64, 9, 9, stride=(2,2))(a)\n",
    "    a = conv_bn_relu(128, 3, 3, stride=(2,2))(a)\n",
    "    for i in range(5):\n",
    "        a = res(128,3,3)(a)\n",
    "    a = dconv_bn_relu(64,3,3)(a)\n",
    "    a = dconv_bn_relu(32,3,3)(a)\n",
    "    y = dconv_bn_relu(3,9,9,stride=(1,1))(a)\n",
    "    return  Model(input=x, output=y)\n",
    "\n",
    "# print image_transform_netl().summary() \n",
    "\n",
    "\n",
    "# Input 3 × 256 × 256\n",
    "# Reflection Padding (40 × 40) 3 × 336 × 336\n",
    "# 32 × 9 × 9 conv, stride 1 32 × 336 × 336\n",
    "# 64 × 3 × 3 conv, stride 2 64 × 168 × 168\n",
    "# 128 × 3 × 3 conv, stride 2 128 × 84 × 84\n",
    "# Residual block, 128 filters 128 × 80 × 80\n",
    "# Residual block, 128 filters 128 × 76 × 76\n",
    "# Residual block, 128 filters 128 × 72 × 72\n",
    "# Residual block, 128 filters 128 × 68 × 68\n",
    "# Residual block, 128 filters 128 × 64 × 64\n",
    "# 64 × 3 × 3 conv, stride 1/2 64 × 128 × 128\n",
    "# 32 × 3 × 3 conv, stride 1/2 32 × 256 × 256\n",
    "# 3 × 9 × 9 conv, stride 1 3 × 256 × 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from scipy.misc import imsave\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from keras.applications import vgg16\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg16.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        x = x.reshape((3, img_nrows, img_ncols))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        features = K.batch_flatten(x)\n",
    "    else:\n",
    "        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "# the \"style loss\" is designed to maintain\n",
    "# the style of the reference image in the generated image.\n",
    "# It is based on the gram matrices (which capture style) of\n",
    "# feature maps from the style reference image\n",
    "# and from the generated image\n",
    "\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(combination) == 3\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
    "\n",
    "# an auxiliary loss function\n",
    "# designed to maintain the \"content\" of the\n",
    "# base image in the generated image\n",
    "\n",
    "\n",
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))\n",
    "\n",
    "# the 3rd loss function, total variation loss,\n",
    "# designed to keep the generated image locally coherent\n",
    "\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    assert K.ndim(x) == 4\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        a = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n",
    "        b = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n",
    "    else:\n",
    "        a = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
    "        b = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_image_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-6fe549937f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# get tensor representations of our images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbase_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstyle_reference_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_reference_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_image_path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# get tensor representations of our images\n",
    "base_image = K.variable(preprocess_image(base_image_path))\n",
    "style_reference_image = K.variable(preprocess_image(style_reference_image_path))\n",
    "combination_image = K.placeholder((1, img_nrows, img_ncols, 3))\n",
    "    \n",
    "\n",
    "# combine the 3 images into a single Keras tensor\n",
    "input_tensor = K.concatenate([base_image,\n",
    "                              style_reference_image,\n",
    "                              combination_image], axis=0)\n",
    "\n",
    "# build the VGG16 network with our 3 images as input\n",
    "# the model will be loaded with pre-trained ImageNet weights\n",
    "model = vgg16.VGG16(input_tensor=input_tensor,\n",
    "                    weights='imagenet', include_top=False)\n",
    "\n",
    "\n",
    "# combine these loss functions into a single scalar\n",
    "loss = K.variable(0.)\n",
    "layer_features = outputs_dict['block4_conv2']\n",
    "base_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "loss += content_weight * content_loss(base_image_features,\n",
    "                                      combination_features)\n",
    "\n",
    "feature_layers = ['block1_conv1', 'block2_conv1',\n",
    "                  'block3_conv1', 'block4_conv1',\n",
    "                  'block5_conv1']\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    sl = style_loss(style_reference_features, combination_features)\n",
    "    loss += (style_weight / len(feature_layers)) * sl\n",
    "loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "\n",
    "\n",
    "def loss_net():\n",
    "    x = Input(shape=(256,256,3))\n",
    "    a = ReflectionPadding2D(padding=(40,40),input_shape=(256,256,3))(x)\n",
    "    a = conv_bn_relu(32, 9, 9, stride=(1,1))(a)\n",
    "    a = conv_bn_relu(64, 9, 9, stride=(2,2))(a)\n",
    "    a = conv_bn_relu(128, 3, 3, stride=(2,2))(a)\n",
    "    for i in range(5):\n",
    "        a = res(128,3,3)(a)\n",
    "    a = dconv_bn_relu(64,3,3)(a)\n",
    "    a = dconv_bn_relu(32,3,3)(a)\n",
    "    y = dconv_bn_relu(3,9,9,stride=(1,1))(a)\n",
    "    return  Model(input=x, output=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
