{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD,Adam\n",
    "from keras.layers import Input, merge\n",
    "from keras.models import Model,Sequential\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "from layers import VGGNormalize,ReflectionPadding2D,Denormalize,conv_bn_relu,res_conv,dconv_bn_relu\n",
    "from loss import StyleReconstructionRegularizer,FeatureReconstructionRegularizer\n",
    "import img_util\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%reload_ext autoreload\n",
    "\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Input 3 × 256 × 256\n",
    "Reflection Padding (40 × 40) 3 × 336 × 336\n",
    "32 × 9 × 9 conv, stride 1 32 × 336 × 336\n",
    "64 × 3 × 3 conv, stride 2 64 × 168 × 168\n",
    "128 × 3 × 3 conv, stride 2 128 × 84 × 84\n",
    "Residual block, 128 filters 128 × 80 × 80\n",
    "Residual block, 128 filters 128 × 76 × 76\n",
    "Residual block, 128 filters 128 × 72 × 72\n",
    "Residual block, 128 filters 128 × 68 × 68\n",
    "Residual block, 128 filters 128 × 64 × 64\n",
    "64 × 3 × 3 conv, stride 1/2 64 × 128 × 128\n",
    "32 × 3 × 3 conv, stride 1/2 32 × 256 × 256\n",
    "3 × 9 × 9 conv, stride 1 3 × 256 × 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def image_transform_net():\n",
    "    x = Input(shape=(256,256,3))\n",
    "    a = ReflectionPadding2D(padding=(40,40),input_shape=(256,256,3))(x)\n",
    "    a = conv_bn_relu(32, 9, 9, stride=(1,1))(a)\n",
    "    a = conv_bn_relu(64, 9, 9, stride=(2,2))(a)\n",
    "    a = conv_bn_relu(128, 3, 3, stride=(2,2))(a)\n",
    "    for i in range(5):\n",
    "        a = res_conv(128,3,3)(a)\n",
    "    a = dconv_bn_relu(64,3,3)(a)\n",
    "    a = dconv_bn_relu(32,3,3)(a)\n",
    "    a = dconv_bn_relu(3,9,9,stride=(1,1))(a)\n",
    "    # Scale output to range [0, 255] via custom Denormalize layer\n",
    "    y = Denormalize(name='transform_output')(a)\n",
    "    return  Model(input=x, output=y)\n",
    "\n",
    "# print image_transform_net().summary() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from scipy.misc import imsave\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG Model weights loaded.\n"
     ]
    }
   ],
   "source": [
    "from VGG16 import vgg16\n",
    "\n",
    "def loss_net(x_in, trux_x_in):\n",
    "    # Append the initial input to the FastNet input to the VGG inputs\n",
    "    x = merge([x_in, trux_x_in], mode='concat', concat_axis=0)\n",
    "\n",
    "    # Normalize the inputs via custom VGG Normalization layer\n",
    "    x = VGGNormalize(name=\"vgg_normalize\")(x)\n",
    "\n",
    "    vgg = vgg16(include_top=False,input_tensor=x)\n",
    "\n",
    "    vgg_output_dict = dict([(layer.name, layer.output) for layer in model.layers[-18:]])\n",
    "    vgg_layers = dict([(layer.name, layer) for layer in model.layers[-18:]])\n",
    "    \n",
    "    # Freeze all VGG layers\n",
    "    for layer in vgg.layers[-19:]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return vgg\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "model = image_transform_net()\n",
    "#if tranning\n",
    "model = loss_net(model.output,model.input)\n",
    "\n",
    "# print model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting style features from VGG network.\n",
      "Tensor(\"strided_slice_243:0\", shape=(), dtype=int32) Tensor(\"strided_slice_242:0\", shape=(), dtype=int32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor conversion requested dtype float32 for Tensor with dtype int32: 'Tensor(\"mul_143:0\", shape=(), dtype=int32)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-3ec2ed60b61b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontent_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m content_regularizer = FeatureReconstructionRegularizer(\n\u001b[0;32m---> 54\u001b[0;31m                                    weight=content_weight)(layer)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/fast-neural-style-keras/loss.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    649\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    587\u001b[0m     raise ValueError(\n\u001b[1;32m    588\u001b[0m         \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         % (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m    590\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype float32 for Tensor with dtype int32: 'Tensor(\"mul_143:0\", shape=(), dtype=int32)'"
     ]
    }
   ],
   "source": [
    "img_width = 256\n",
    "img_height = 256\n",
    "style_weight=5.\n",
    "content_weight=1.\n",
    "tv_weight=1e3\n",
    "style_image_path = \"images/style/starry_night.jpg\"\n",
    "\n",
    "\n",
    "def get_vgg_style_features(input_img,model,style_layer_outputs):\n",
    "    vgg_style_func = K.function([model.layers[-19].input], style_layer_outputs)\n",
    "\n",
    "    return vgg_style_func([input_img])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vgg_output_dict = dict([(layer.name, layer.output) for layer in model.layers[-18:]])\n",
    "vgg_layers = dict([(layer.name, layer) for layer in model.layers[-18:]])\n",
    "\n",
    "style = img_util.preprocess_image(style_image_path, img_width, img_height)\n",
    "print('Getting style features from VGG network.')\n",
    "\n",
    "style_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3']\n",
    "\n",
    "style_layer_outputs = []\n",
    "\n",
    "for layer in style_layers:\n",
    "    style_layer_outputs.append(vgg_output_dict[layer])\n",
    "\n",
    "style_features = get_vgg_style_features(style,model,style_layer_outputs)\n",
    "\n",
    "\n",
    "\n",
    "# Style Reconstruction Loss\n",
    "for i, layer_name in enumerate(style_layers):\n",
    "    layer = vgg_layers[layer_name]\n",
    "\n",
    "    \n",
    "    feature_var = K.variable(value=style_features[i][0])\n",
    "    style_loss = StyleReconstructionRegularizer(\n",
    "                        style_feature_target=feature_var,\n",
    "                        weight=style_weight)(layer)\n",
    "\n",
    "    layer.add_loss(style_loss)\n",
    "\n",
    "    \n",
    "    \n",
    "# Feature Reconstruction Loss\n",
    "content_layer = 'block4_conv2'\n",
    "content_layer_output = vgg_output_dict[content_layer]\n",
    "\n",
    "layer = vgg_layers[content_layer]\n",
    "content_regularizer = FeatureReconstructionRegularizer(\n",
    "                                   weight=content_weight)(layer)\n",
    "layer.add_loss(content_regularizer)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#             # Feature Reconstruction Loss\n",
    "#             self.content_layer = 'conv4_2'\n",
    "#             self.content_layer_output = self.vgg_output_dict[self.content_layer]\n",
    "\n",
    "#             if self.content_weight != 0.0:\n",
    "#                 layer = vgg_layers[self.content_layer]\n",
    "#                 content_regularizer = FeatureReconstructionRegularizer(\n",
    "#                     weight=self.content_weight)(layer)\n",
    "#                 layer.add_loss(content_regularizer)\n",
    "\n",
    "#         # Total Variation Regularization\n",
    "#         if self.tv_weight != 0.0:\n",
    "#             layer = fastnet_output_layer  # Fastnet Output layer\n",
    "#             tv_regularizer = TVRegularizer(img_width=self.img_width, img_height=self.img_height,\n",
    "#                                            weight=self.tv_weight)(layer)\n",
    "#             layer.add_loss(tv_regularizer)\n",
    "\n",
    "#         if self.model is None:\n",
    "#             self.model = model\n",
    "#         return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_image_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-6fe549937f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# get tensor representations of our images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbase_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstyle_reference_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_reference_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_image_path' is not defined"
     ]
    }
   ],
   "source": [
    "base_image_path = \n",
    "# get tensor representations of our images\n",
    "base_image = K.variable(preprocess_image(base_image_path))\n",
    "style_reference_image = K.variable(preprocess_image(style_reference_image_path))\n",
    "combination_image = K.placeholder((1, img_nrows, img_ncols, 3))\n",
    "    \n",
    "\n",
    "# combine the 3 images into a single Keras tensor\n",
    "input_tensor = K.concatenate([base_image,\n",
    "                              style_reference_image,\n",
    "                              combination_image], axis=0)\n",
    "\n",
    "# build the VGG16 network with our 3 images as input\n",
    "# the model will be loaded with pre-trained ImageNet weights\n",
    "model = vgg16.VGG16(input_tensor=input_tensor,\n",
    "                    weights='imagenet', include_top=False)\n",
    "\n",
    "\n",
    "# combine these loss functions into a single scalar\n",
    "loss = K.variable(0.)\n",
    "layer_features = outputs_dict['block4_conv2']\n",
    "base_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "loss += content_weight * content_loss(base_image_features,\n",
    "                                      combination_features)\n",
    "\n",
    "feature_layers = ['block1_conv1', 'block2_conv1',\n",
    "                  'block3_conv1', 'block4_conv1',\n",
    "                  'block5_conv1']\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    sl = style_loss(style_reference_features, combination_features)\n",
    "    loss += (style_weight / len(feature_layers)) * sl\n",
    "loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "\n",
    "\n",
    "def loss_net():\n",
    "    x = Input(shape=(256,256,3))\n",
    "    a = ReflectionPadding2D(padding=(40,40),input_shape=(256,256,3))(x)\n",
    "    a = conv_bn_relu(32, 9, 9, stride=(1,1))(a)\n",
    "    a = conv_bn_relu(64, 9, 9, stride=(2,2))(a)\n",
    "    a = conv_bn_relu(128, 3, 3, stride=(2,2))(a)\n",
    "    for i in range(5):\n",
    "        a = res(128,3,3)(a)\n",
    "    a = dconv_bn_relu(64,3,3)(a)\n",
    "    a = dconv_bn_relu(32,3,3)(a)\n",
    "    y = dconv_bn_relu(3,9,9,stride=(1,1))(a)\n",
    "    return  Model(input=x, output=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
